# CS149 Lecture 1: Why Parallelism? Why Efficiency?

---

## 1. Course Overview（课程简介）

- **Course Title**: CS149: Parallel Computing（并行计算）
- **Key Objectives**:
  - Understand why **parallelism**（并行性） is essential in modern computing.
  - Learn to design and implement **parallel programs**（并行程序）.
  - Master performance analysis and tradeoffs in **parallel architectures**（并行架构）.

---

## 2. Why Parallelism?（为什么要并行？）

### 2.1 Limitations of Single-thread Performance（单线程性能的瓶颈）

- **Moore’s Law**（摩尔定律） previously ensured that performance doubled every 18 months.
- **Dennard Scaling**（丹纳德缩放） failed in the mid-2000s due to power constraints.
- Frequency scaling stopped at ~3GHz due to **thermal and power limits**（热功率限制）.

📌 Conclusion: We can no longer rely on faster single cores. We must go **parallel**.

---

### 2.2 Limits of ILP: Instruction-Level Parallelism（指令级并行性的限制）

- **ILP** extracts parallelism within a single thread using pipelining, out-of-order execution, etc.
- ILP improvements have diminishing returns. There's a limit to how much can be extracted.

---

### 2.3 Rise of Multicore and Parallel Hardware（多核与并行硬件的兴起）

- Modern CPUs have multiple **cores**（内核） that execute tasks concurrently.
- **GPUs**（图形处理器） provide thousands of **lightweight threads**（轻量级线程） for data-parallel tasks.

🧠 Software must be rewritten to take advantage of these parallel capabilities.

---

## 3. Challenges in Parallel Programming（并行编程的挑战）

### 3.1 Communication Overhead（通信开销）

- Threads or processes need to **synchronize and communicate**（同步与通信）.
- Data movement can dominate execution time, especially in **distributed systems**（分布式系统）.

### 3.2 Load Balancing（负载均衡）

- Tasks must be distributed evenly among processing units.
- Imbalance leads to **idle cores**（空闲内核） and wasted resources.

### 3.3 Synchronization and Data Races（同步与数据竞争）

- Threads must avoid accessing shared data unsafely.
- Use mechanisms like **locks**, **barriers**, and **atomic operations**（锁、屏障、原子操作）.

---

## 4. Thinking in Parallel（并行思维）

To write efficient parallel code:

1. **Decompose** (分解): Break tasks into **independent subtasks**（独立子任务）.
2. **Distribute** (分发): Assign subtasks to threads or cores.
3. **Coordinate** (协调): Handle communication, dependencies, and synchronization.

🧩 Abstractions like **data parallelism**（数据并行） and **task parallelism**（任务并行） are critical.

---

## 5. Hardware Trends（硬件趋势）

- CPU: Fewer, more powerful cores + **SIMD**（单指令多数据） units
- GPU: Many lightweight cores optimized for **data-parallel** workloads
- Accelerator: TPUs, FPGAs, ASICs for **domain-specific computation**（特定领域计算）

---

## 6. Efficiency ≠ Speed（效率 ≠ 速度）

- A program may run fast but waste power or under-utilize hardware.
- Efficiency considers **throughput**, **latency**, **utilization**, and **energy consumption**（吞吐量、延迟、利用率、能耗）.

---

## 7. Course Logistics（课程安排）

- **Programming Assignments**: 4 total, 48% of final grade
- **Written Homework**: 5 sets, 15%
- **Midterms**: 2 tests, 16%
- **Final Exam**: 16%
- **Participation (Piazza Posts)**: 5%

---

## 8. Summary（总结）

✅ Parallel computing is the **only path forward** in modern computing  
✅ Requires rethinking how we design and write software  
✅ This course teaches the **foundations of parallelism** from hardware to code

---

## 🧠 Key Terms Glossary

| Term                  | 中文说明             |
|-----------------------|----------------------|
| Parallelism           | 并行性               |
| Instruction-Level Parallelism (ILP) | 指令级并行性     |
| Synchronization       | 同步                 |
| Load Balancing        | 负载均衡             |
| Data Race             | 数据竞争             |
| GPU                   | 图形处理器           |
| Thread                | 线程                 |
| Latency               | 延迟                 |
| Throughput            | 吞吐量               |

---

