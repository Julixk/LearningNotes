# CS149 Lecture 1: Why Parallelism? Why Efficiency?

---

## 1. Course Overviewï¼ˆè¯¾ç¨‹ç®€ä»‹ï¼‰

- **Course Title**: CS149: Parallel Computingï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰
- **Key Objectives**:
  - Understand why **parallelism**ï¼ˆå¹¶è¡Œæ€§ï¼‰ is essential in modern computing.
  - Learn to design and implement **parallel programs**ï¼ˆå¹¶è¡Œç¨‹åºï¼‰.
  - Master performance analysis and tradeoffs in **parallel architectures**ï¼ˆå¹¶è¡Œæ¶æ„ï¼‰.

---

## 2. Why Parallelism?ï¼ˆä¸ºä»€ä¹ˆè¦å¹¶è¡Œï¼Ÿï¼‰

### 2.1 Limitations of Single-thread Performanceï¼ˆå•çº¿ç¨‹æ€§èƒ½çš„ç“¶é¢ˆï¼‰

- **Mooreâ€™s Law**ï¼ˆæ‘©å°”å®šå¾‹ï¼‰ previously ensured that performance doubled every 18 months.
- **Dennard Scaling**ï¼ˆä¸¹çº³å¾·ç¼©æ”¾ï¼‰ failed in the mid-2000s due to power constraints.
- Frequency scaling stopped at ~3GHz due to **thermal and power limits**ï¼ˆçƒ­åŠŸç‡é™åˆ¶ï¼‰.

ğŸ“Œ Conclusion: We can no longer rely on faster single cores. We must go **parallel**.

---

### 2.2 Limits of ILP: Instruction-Level Parallelismï¼ˆæŒ‡ä»¤çº§å¹¶è¡Œæ€§çš„é™åˆ¶ï¼‰

- **ILP** extracts parallelism within a single thread using pipelining, out-of-order execution, etc.
- ILP improvements have diminishing returns. There's a limit to how much can be extracted.

---

### 2.3 Rise of Multicore and Parallel Hardwareï¼ˆå¤šæ ¸ä¸å¹¶è¡Œç¡¬ä»¶çš„å…´èµ·ï¼‰

- Modern CPUs have multiple **cores**ï¼ˆå†…æ ¸ï¼‰ that execute tasks concurrently.
- **GPUs**ï¼ˆå›¾å½¢å¤„ç†å™¨ï¼‰ provide thousands of **lightweight threads**ï¼ˆè½»é‡çº§çº¿ç¨‹ï¼‰ for data-parallel tasks.

ğŸ§  Software must be rewritten to take advantage of these parallel capabilities.

---

## 3. Challenges in Parallel Programmingï¼ˆå¹¶è¡Œç¼–ç¨‹çš„æŒ‘æˆ˜ï¼‰

### 3.1 Communication Overheadï¼ˆé€šä¿¡å¼€é”€ï¼‰

- Threads or processes need to **synchronize and communicate**ï¼ˆåŒæ­¥ä¸é€šä¿¡ï¼‰.
- Data movement can dominate execution time, especially in **distributed systems**ï¼ˆåˆ†å¸ƒå¼ç³»ç»Ÿï¼‰.

### 3.2 Load Balancingï¼ˆè´Ÿè½½å‡è¡¡ï¼‰

- Tasks must be distributed evenly among processing units.
- Imbalance leads to **idle cores**ï¼ˆç©ºé—²å†…æ ¸ï¼‰ and wasted resources.

### 3.3 Synchronization and Data Racesï¼ˆåŒæ­¥ä¸æ•°æ®ç«äº‰ï¼‰

- Threads must avoid accessing shared data unsafely.
- Use mechanisms like **locks**, **barriers**, and **atomic operations**ï¼ˆé”ã€å±éšœã€åŸå­æ“ä½œï¼‰.

---

## 4. Thinking in Parallelï¼ˆå¹¶è¡Œæ€ç»´ï¼‰

To write efficient parallel code:

1. **Decompose** (åˆ†è§£): Break tasks into **independent subtasks**ï¼ˆç‹¬ç«‹å­ä»»åŠ¡ï¼‰.
2. **Distribute** (åˆ†å‘): Assign subtasks to threads or cores.
3. **Coordinate** (åè°ƒ): Handle communication, dependencies, and synchronization.

ğŸ§© Abstractions like **data parallelism**ï¼ˆæ•°æ®å¹¶è¡Œï¼‰ and **task parallelism**ï¼ˆä»»åŠ¡å¹¶è¡Œï¼‰ are critical.

---

## 5. Hardware Trendsï¼ˆç¡¬ä»¶è¶‹åŠ¿ï¼‰

- CPU: Fewer, more powerful cores + **SIMD**ï¼ˆå•æŒ‡ä»¤å¤šæ•°æ®ï¼‰ units
- GPU: Many lightweight cores optimized for **data-parallel** workloads
- Accelerator: TPUs, FPGAs, ASICs for **domain-specific computation**ï¼ˆç‰¹å®šé¢†åŸŸè®¡ç®—ï¼‰

---

## 6. Efficiency â‰  Speedï¼ˆæ•ˆç‡ â‰  é€Ÿåº¦ï¼‰

- A program may run fast but waste power or under-utilize hardware.
- Efficiency considers **throughput**, **latency**, **utilization**, and **energy consumption**ï¼ˆååé‡ã€å»¶è¿Ÿã€åˆ©ç”¨ç‡ã€èƒ½è€—ï¼‰.

---

## 7. Course Logisticsï¼ˆè¯¾ç¨‹å®‰æ’ï¼‰

- **Programming Assignments**: 4 total, 48% of final grade
- **Written Homework**: 5 sets, 15%
- **Midterms**: 2 tests, 16%
- **Final Exam**: 16%
- **Participation (Piazza Posts)**: 5%

---

## 8. Summaryï¼ˆæ€»ç»“ï¼‰

âœ… Parallel computing is the **only path forward** in modern computing  
âœ… Requires rethinking how we design and write software  
âœ… This course teaches the **foundations of parallelism** from hardware to code

---

## ğŸ§  Key Terms Glossary

| Term                  | ä¸­æ–‡è¯´æ˜             |
|-----------------------|----------------------|
| Parallelism           | å¹¶è¡Œæ€§               |
| Instruction-Level Parallelism (ILP) | æŒ‡ä»¤çº§å¹¶è¡Œæ€§     |
| Synchronization       | åŒæ­¥                 |
| Load Balancing        | è´Ÿè½½å‡è¡¡             |
| Data Race             | æ•°æ®ç«äº‰             |
| GPU                   | å›¾å½¢å¤„ç†å™¨           |
| Thread                | çº¿ç¨‹                 |
| Latency               | å»¶è¿Ÿ                 |
| Throughput            | ååé‡               |

---

